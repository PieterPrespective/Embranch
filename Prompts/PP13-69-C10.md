- IssueID = PP13-69-C10
- Please read 'Prompts/BasePrompt.md' first for general context
- The PP13_68_PerformanceTests integration tests cause complete test runner deadlock, blocking all test execution when included. These tests validate critical PP13-68 content-hash verification performance but are currently unsuitable for CI/CD due to missing timeout guards and excessive Python operation volume.
- For context please read:
	- The PP13-69 architecture overview in 'Prompts/pp13-69.md' - the foundational design for SQLite sync state architecture
	- The PP13-69-C1 through PP13-69-C9 achievements logged in the chroma design planning database - progressive architectural improvements
	- The performance improvement recommendations in 'Examples/DoltChromaMergeConflicts.md' starting at line 92 - Python.NET optimization strategies
	- The existing deadlock prevention patterns in 'multidolt-mcp-testing/IntegrationTests/PythonNetDeadlockTest.cs' - demonstrates proper timeout usage
- This assignment specifically focuses on making the PP13_68_PerformanceTests safe and runnable without causing test runner deadlock:

## **Problem Analysis - Complete Test Runner Deadlock**

### **Issue 1: No Test-Level Timeouts (Critical)**
**Tests Affected**: All 3 tests in PP13_68_PerformanceTests.cs
**Problem**: None of the tests have `[Timeout]` attributes. When Python operations hang, NUnit waits indefinitely.
**Evidence**: Compare to `PythonNetDeadlockTest.cs:37` which properly uses `[Timeout(30000)]`
**Impact**: A single stuck Python operation causes infinite wait, requiring manual process termination

### **Issue 2: Excessive Python Operation Volume (Critical)**
**Test**: `PP13_68_ContentHashPerformance_VariousDocumentCounts`
**Problem**: Tests 10, 50, 100 documents with ~15 Python operations each = 45+ total operations
**Location**: Lines 141-238 in PP13_68_PerformanceTests.cs
**Impact**: Queue saturation, 30-second timeouts per operation can compound to 20+ minutes

### **Issue 3: Memory Test Cumulative Load (Critical)**
**Test**: `PP13_68_ContentHashMemoryUsage_StabilityTest`
**Problem**: 10 iterations Ã— 20 documents = 200 documents with 10 branch switches = 100+ Python operations
**Location**: Lines 346-422 in PP13_68_PerformanceTests.cs
**Impact**: Exponential load increase causes GIL contention and memory pressure

### **Issue 4: Synchronous TearDown Blocking (Secondary)**
**Problem**: TearDown uses `.GetAwaiter().GetResult()` which deadlocks if Python queue is backed up
**Location**: Lines 104-133 in PP13_68_PerformanceTests.cs
**Impact**: Even if test times out, cleanup can deadlock indefinitely

### **Root Cause Summary**
The deadlock occurs because:
1. Tests queue many Python operations rapidly (embedding computation is CPU intensive)
2. ChromaDB embedding computation for large batches exceeds 30-second default timeout
3. Python thread holds GIL during embedding, blocking all queued operations
4. No test timeout means NUnit waits forever
5. TearDown attempts sync operations on saturated queue, causing secondary deadlock

## **Assignment Objectives**

### **Primary Goals:**
1. **Add Timeout Guards**: Add `[Timeout]` attributes to all performance tests to prevent infinite waits
2. **Reduce Test Load**: Lower document counts and iterations to achievable levels that complete within timeouts
3. **Fix TearDown**: Make cleanup resilient to Python queue saturation
4. **Preserve Test Value**: Maintain meaningful performance validation despite reduced load

### **Success Criteria:**
```
PP13_68_PerformanceTests tests: 3
     Passed: 3 (or skipped with clear reason)
     Failed: 0
     Deadlocked: 0
     Total time: < 5 minutes
```

### **Implementation Requirements:**

1. **Add Timeout Attributes to All Tests**:
   ```csharp
   [Test]
   [Timeout(120000)] // 2 minutes - sufficient for reduced load
   public async Task PP13_68_ContentHashPerformance_VariousDocumentCounts()

   [Test]
   [Timeout(120000)] // 2 minutes
   public async Task PP13_68_ContentHashPerformance_VariousDocumentSizes()

   [Test]
   [Timeout(180000)] // 3 minutes - memory test needs slightly longer
   public async Task PP13_68_ContentHashMemoryUsage_StabilityTest()
   ```

2. **Reduce Document Counts (Test 1)**:
   ```csharp
   // BEFORE: Excessive load
   var documentCounts = new[] { 10, 50, 100 };

   // AFTER: Achievable load that still validates performance
   var documentCounts = new[] { 5, 10, 20 };
   ```

   **Rationale**:
   - 5 documents: Baseline minimum for meaningful measurement
   - 10 documents: Moderate load validation
   - 20 documents: Reasonable stress test within timeout constraints
   - Total operations reduced from 45+ to ~30

3. **Reduce Memory Test Iterations (Test 3)**:
   ```csharp
   // BEFORE: Excessive iterations
   const int iterations = 10;
   const int documentsPerIteration = 20;

   // AFTER: Achievable load
   const int iterations = 3;
   const int documentsPerIteration = 5;
   ```

   **Rationale**:
   - 3 iterations sufficient to detect memory leaks (growth pattern visible)
   - 5 documents per iteration reduces Python operation count by 75%
   - Total documents: 15 instead of 200
   - Total Python operations: ~30 instead of 100+

4. **Adjust Performance Thresholds**:
   ```csharp
   // Update thresholds to match reduced document counts
   // The assertions should still validate performance is acceptable
   // but with realistic expectations for smaller workloads

   // Example: Documents per second threshold can remain at 5.0
   // since smaller batches should be MORE efficient, not less
   Assert.That(docsPerSec, Is.GreaterThan(5.0), ...);
   ```

5. **Fix TearDown to Be Resilient**:
   ```csharp
   [TearDown]
   public async Task TearDown()
   {
       try
       {
           // Use timeout-protected cleanup
           using var cts = new CancellationTokenSource(TimeSpan.FromSeconds(10));

           var collections = await _chromaService?.ListCollectionsAsync();
           if (collections != null)
           {
               foreach (var collection in collections.Where(c => c.StartsWith("pp13_68_perf_")))
               {
                   try
                   {
                       // Individual cleanup with short timeout
                       var deleteTask = _chromaService.DeleteCollectionAsync(collection);
                       if (await Task.WhenAny(deleteTask, Task.Delay(5000)) != deleteTask)
                       {
                           // Cleanup timed out, log and continue
                           break;
                       }
                   }
                   catch { /* Ignore cleanup errors */ }
               }
           }
       }
       catch (Exception ex)
       {
           // Log but don't fail on cleanup errors
           Console.WriteLine($"TearDown warning: {ex.Message}");
       }
       finally
       {
           // Always attempt to dispose ChromaService
           try { _chromaService?.Dispose(); } catch { }
       }
   }
   ```

6. **Add Category Attribute for CI Flexibility**:
   ```csharp
   [TestFixture]
   [Category("Performance")]
   public class PP13_68_PerformanceTests
   ```

   This allows CI/CD to exclude with `--filter "Category!=Performance"` if needed.

### **Technical Constraints:**
- **DO NOT** remove the tests entirely - they validate important PP13-68 functionality
- **MAINTAIN** the core performance validation logic (timing measurements, assertions)
- **PRESERVE** the test structure (setup, multiple document counts, performance assertions)
- **ENSURE** tests remain meaningful despite reduced load (performance ratios still valid)
- **DO NOT** modify production code - only test code changes

### **Performance Assertion Guidance:**

The reduced document counts should NOT require relaxing performance thresholds because:
- Smaller batches have LESS overhead, not more
- Python.NET GIL contention is lower with fewer operations
- The per-document metrics (ms/doc, docs/sec) should be BETTER with smaller batches

If tests fail performance assertions after load reduction:
1. This indicates a genuine performance regression
2. Investigate before relaxing thresholds
3. Only relax if there's a valid architectural reason

### **Validation Process:**
1. Run each test individually first to verify timeout behavior works
2. Verify tests complete within timeout limits
3. Confirm performance assertions still pass (smaller load = same or better per-doc performance)
4. Run all 3 tests together to verify no cumulative issues
5. Run alongside other test suites to ensure no interference
6. Verify TearDown completes even when tests timeout

## **Expected Outcome**
All PP13_68_PerformanceTests complete reliably without deadlock:
- Tests execute within timeout limits (2-3 minutes each)
- Performance validation remains meaningful
- TearDown completes even after timeout/failure
- CI/CD can run these tests safely (or exclude via category filter)
- No degradation to PP13-68 content-hash verification confidence

**Priority**: **High** - These tests block full test suite execution when they deadlock, preventing validation of all other functionality.

## **Files to be Modified**
- `multidolt-mcp-testing/IntegrationTests/PP13_68_PerformanceTests.cs` - All changes concentrated here:
  - Add `[Timeout]` attributes to all 3 tests
  - Add `[Category("Performance")]` to test fixture
  - Reduce document counts in Test 1
  - Reduce iterations and documents in Test 3
  - Convert TearDown to async with timeout protection

## **Testing Verification**
After implementation, verify with:
```bash
# Run individual tests
dotnet test --filter "FullyQualifiedName~PP13_68_ContentHashPerformance_VariousDocumentCounts" --logger "console;verbosity=normal"
dotnet test --filter "FullyQualifiedName~PP13_68_ContentHashPerformance_VariousDocumentSizes" --logger "console;verbosity=normal"
dotnet test --filter "FullyQualifiedName~PP13_68_ContentHashMemoryUsage_StabilityTest" --logger "console;verbosity=normal"

# Run all performance tests together
dotnet test --filter "FullyQualifiedName~PP13_68_PerformanceTests" --logger "console;verbosity=normal"

# Verify can be excluded in CI
dotnet test --filter "Category!=Performance" --list-tests
```

Please log all development actions to the 'chroma-feat-design-planning-mcp' database under collection 'PP13-69-C10' following the established pattern.
